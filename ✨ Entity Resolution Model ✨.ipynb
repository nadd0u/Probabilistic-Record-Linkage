{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7cc35f4-02de-4de2-9fd6-dc53d6c81f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'splink.spark'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 👾 IMPORTS 👾\n",
    "from splink.spark.jar_location import similarity_jar_location\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, types\n",
    "import pandas as pd\n",
    "import string\n",
    "from splink.spark.linker import SparkLinker\n",
    "import splink.spark.comparison_library as cl\n",
    "import splink.spark.comparison_template_library as ctl\n",
    "from splink.spark.blocking_rule_library import block_on\n",
    "import splink.comparison_helpers as ch\n",
    "import splink.duckdb.comparison_template_library as clt\n",
    "conf = SparkConf()\n",
    "\n",
    "path = similarity_jar_location()\n",
    "conf.set(\"spark.jars\", path)\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp_checkpoints/r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1011b9f2-ab5a-4213-b4de-c075bbf480f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG development;\n",
    "USE SCHEMA splink_training_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73754645-836b-42c7-b59e-63703fb0bc31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Intent: \n",
    "We are training a `probabilistic record linkage` model to identify records that refer to the same person across different datasets or databases. \n",
    "\n",
    "This model will be used to reduce duplicates and consolidate information from various data sources, ensuring that person entities are stored under a single unique identifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141aec55-8f2e-4dbb-907f-d0049883541b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 👾 INPUTS 👾\n",
    "TRAINING_DATA = 'development.silver.entity_resolution_training_data'\n",
    "\n",
    "# 👾 OUTPUTS 👾\n",
    "MODEL = '/dbfs/FileStore/splink_saved_json_files/entity_resolution_model_train.json'\n",
    "PREDICTION_TABLE = 'development.splink_training_data.entity_resolution_training_predictions'\n",
    "CLUSTER_TABLE = 'development.splink_training_data.entity_resolution_training_clusters'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc61443a-c7b7-4da3-a099-60a4259eee79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾  Training Data  👾\n",
    "\n",
    "Training data is cleaned & prepared according to our [pre-processing requirements](https://dbc-cc8b3cdd-9625.cloud.databricks.com/?o=3471402352352814#notebook/3188833296771907/command/3188833296771908). \n",
    "\n",
    "**Training Data Considerations** </br>\n",
    " ```\n",
    "💥 Data should be a random sample of the population. Check for bias introduced through your sampling technique.\n",
    "💥 The Expectation Maximisation Algorithm works best when the pariwise record comparisons are a mix of between 0.1% and 0.99% true matches.\n",
    "💥 Ensure that the dataset contains sufficient information to distinguish between records. \n",
    "💥 Your sample should represent the full range of variability in your dataset. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e43d8a24-1ac8-479b-bd04-d779fe036fe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    contact_id as unique_id, \n",
    "    full_name,\n",
    "    first_name,\n",
    "    last_name, \n",
    "    last_name_alt_2,\n",
    "    nickname, \n",
    "    soundex(first_name) as phonetic_first_name,\n",
    "    soundex(last_name) as phonetic_last_name,\n",
    "    soundex(full_name) as phonetic_full_name\n",
    "    street_number,\n",
    "    street_name, \n",
    "    unit_number,\n",
    "    CONCAT(street_number,' ',street_name, ' ', coalesce(unit_number, '')) AS full_address,\n",
    "    CONCAT(street_number,' ',street_name) AS address_1,\n",
    "    zip5,\n",
    "    age, \n",
    "    gender,\n",
    "    birth_month,\n",
    "    birth_day\n",
    "FROM {TRAINING_DATA}\n",
    "\"\"\"\n",
    "df = sqlContext.sql(query)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643e38c7-0a7b-4585-b2a5-d1c731492d62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### EVIDENCE OF MATCH \n",
    "🤖`----------------`🤖`--------------------`🤖`--------------------`🤖 </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee4caa5-735f-40e7-8cf2-6f0bf9f7ab21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**👾  comparison vector design  👾**</br>\n",
    "The comparison vector is our measure of similarity between two records (or evidence of a match). It is comprised of `comparisons` and `comparison levels`.\n",
    "\n",
    "\n",
    "**Comparisons:** *what to compare?*  </br>\n",
    "`comparisons` are the features we compare between records to find evidence for a match. </br>\n",
    "\n",
    "```\n",
    "💥 Choose features that provide meaningful evidence for distinguishing matches from non matches.\n",
    "💥 Ensure each feature offers unique information. \n",
    "💥 Features should possess ambiguity; those that are always exact matches, like zip code, can bias the model.\n",
    "💥 Do not compare correlated features\n",
    "💥 Level of Missingness: features with high % missing are less useful\n",
    "💥 Cardinality: features with higher cardinality are more useful\n",
    "💥 Skew: a high cardinality feature where most data falls into one category is less useful \n",
    "💥 Combine features to create more robust representations and to deal with correlation, low cardinality, high proportion of nulls.\n",
    "```\n",
    "\n",
    "**Comparison Levels:** *how to compare?*  </br>\n",
    "Each `comparison` has associated `levels` expressing a class or measure of similarity and thus, a different type (and amount) of evidence for a match. \n",
    "\n",
    "```\n",
    "💥 Be familiar with the ambiguity in your data\n",
    "💥 Start with similarity measures that are theoretically suitable to the comparison.\n",
    "💥 Begin with few levels and gradually introduce complexity.\n",
    "💥 Each comparison level should have a substantial amount of data. Remove overly specific levels.\n",
    "💥 If a feature is skewed consider setting term_frequency_adjustment = True \n",
    "```\n",
    "🧰 Validate similarity measures against examples: `ch.comparator_score(\"Richard\", \"iRchard\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3551a6df-36ce-42ca-a8d1-af4c3bbbee93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Name Comparison 🏷️\n",
    "comparison_name = {\n",
    "    \"output_column_name\": \"name\",\n",
    "    \"comparison_description\": \"name\",\n",
    "    \"comparison_levels\": [ {\"sql_condition\": \"full_name_l IS NULL OR full_name_r IS NULL\", \"label_for_charts\": \"Null\", \"is_null_level\": True,},\n",
    "                            # ✨ Full Name Exact Match \n",
    "                          {\"sql_condition\": \"full_name_l = full_name_r\", \"label_for_charts\": \"Full Name Exact Match\",},\n",
    "                              # ✨ First Name Last Name Match \n",
    "                          {\"sql_condition\": \"first_name_l = first_name_r and last_name_l = last_name_r\",\"label_for_charts\": \"First Name Last Name Match\",},\n",
    "                          # ✨ First Name Hyphenated Last Name Match  \n",
    "                          {\"sql_condition\": \"first_name_l = first_name_r and (last_name_l = last_name_alt_2_r or last_name_r = last_name_alt_2_l)\",\"label_for_charts\": \"Hyphenated Last Name Match\",},\n",
    "                          # ✨ Reverse First/Last Name Match \n",
    "                          {\"sql_condition\": \"first_name_l = last_name_r and last_name_l = first_name_r\", \"label_for_charts\": \"Reverse Name Match\",},\n",
    "                            # ✨ Nickname Match \n",
    "                          {\"sql_condition\": \"(nickname1_l = first_name_r or nickname1_r = first_name_l or nickname2_l = first_name_r or nickname2_r = first_name_l or nickname3_l = first_name_r or nickname3_r = first_name_l or nickname4_l = first_name_r or nickname4_r = first_name_l) and last_name_l = last_name_r\",\n",
    "                           \"label_for_charts\": \"NickName Match\",},\n",
    "                            # ✨ Phonetic Full Name Match \n",
    "                          {\"sql_condition\": \"(phonetic_first_name_l = phonetic_first_name_r and phonetic_last_name_l = phonetic_last_name_r) or (phonetic_full_name_l = phonetic_full_name_r and phonetic_full_name_l != phonetic_first_name_l and phonetic_full_name_l != phonetic_first_name_r  and jaro_winkler(`last_name_l`, `last_name_r`) > 0.9)\", \"label_for_charts\": \"Phonetic Full Name Match\",},\n",
    "                            # ✨ Jaro Winkler Match \n",
    "                          {\"sql_condition\": \"jaro_winkler(`first_name_l`, `first_name_r`) > 0.85 and (jaro_winkler(`last_name_l`, `last_name_r`) > 0.9 or levenshtein(`last_name_l`, `last_name_r`) <= 2)\", \"label_for_charts\": \"Jaro Winkler\",},\n",
    "                            # ✨ First Name Middle Initial\n",
    "                          {\"sql_condition\": \"first_name_l = first_name_r\",\"label_for_charts\": \"First Name Match\",},\n",
    "                          {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},],\n",
    "}\n",
    "   \n",
    "# Address Comparison 🏠 \n",
    "comparison_address = {\n",
    "    \"output_column_name\": \"address\",\n",
    "    \"comparison_description\": \"address\",\n",
    "    \"comparison_levels\": [{\"sql_condition\": \"full_address_l IS NULL OR full_address_r IS NULL\", \"label_for_charts\": \"Null\", \"is_null_level\": True,},\n",
    "                          # ✨ Full Address Exact Match \n",
    "                          {\"sql_condition\": \"full_address_l = full_address_r\", \"label_for_charts\": \"Full Address Exact Match\",},\n",
    "                           # ✨ Missing address 2\n",
    "                          {\"sql_condition\": \"(address_1_l = address_1_r and unit_number_l is null) or (address_1_l = address_1_r and unit_number_r is null)\", \"label_for_charts\": \"Address 1 Exact Match missing unit number\",},\n",
    "                          {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},],\n",
    "}\n",
    "\n",
    "#  Age Comparison 👵🏻\n",
    "comparison_age = {\n",
    "    \"output_column_name\": \"age\",\n",
    "    \"comparison_description\": \"age\",\n",
    "    \"comparison_levels\": [{\"sql_condition\": \"age_l is NULL or age_r is NULL\", \"label_for_charts\": \"Null\", \"is_null_level\": True,},\n",
    "                          # ✨ Age Exact Match \n",
    "                          {\"sql_condition\": \"age_l = age_r\" , \"label_for_charts\": \"Age Exact Match\",},\n",
    "                          # ✨ Age within 2 years\n",
    "                          {\"sql_condition\": \"ABS(age_l - age_r)<=2\", \"label_for_charts\": \"Age within 2 years\",},\n",
    "                          # ✨ Age within 5 years\n",
    "                          {\"sql_condition\": \"ABS(age_l - age_r)<=5\", \"label_for_charts\": \"Age within 5 years\",},\n",
    "                          {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},],\n",
    "}\n",
    "\n",
    "# Birthdate Comparison 📅\n",
    "comparison_birthdate = {\n",
    "    \"output_column_name\": \"birthdate\",\n",
    "    \"comparison_description\": \"birthdate\",\n",
    "    \"comparison_levels\": [{\"sql_condition\": \"birth_day_l is NULL or birth_day_r is NULL\", \"label_for_charts\": \"Null\", \"is_null_level\": True,},\n",
    "                          # ✨ BDaY Exact Match \n",
    "                          {\"sql_condition\": \"birth_day_l = birth_day_r and birth_month_r = birth_month_l\" , \"label_for_charts\": \"birthday Exact Match\",},\n",
    "                          {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4ca6e0-91b5-4239-9a92-c653c2129a29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**👾  comparison space  👾** </br>\n",
    "We manage the computational cost of conducting pairwise comparisons by using `blocking rules` to subset our comparison space. These rules specify which pairs of records should be included for matching, with the primary goal of reducing the number of comparisons that the computer must perform. It is important to exercise caution and ensure that potential matches are not inadvertently excluded!\n",
    "```\n",
    "💥 Only eliminate records that are certain not to be a match\n",
    "💥 Select features that effectively reduce the dataset size\n",
    "💥 Use consistent and reliable features\n",
    "💥 Avoid redundant blocking conditions \n",
    "```\n",
    "🧰 check the size of your comparison space: `linker.cumulative_num_comparisons_from_blocking_rules_chart()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66606cef-666c-44a2-8fae-c43714a284f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Configuration for comparison vector + comparison space\n",
    "\n",
    "settings = {\"comparisons\": [comparison_name,\n",
    "                            comparison_address,\n",
    "                            comparison_birthdate,\n",
    "                            cl.exact_match(\"gender\"),\n",
    "                            comparison_age],\n",
    "            \"link_type\": \"dedupe_only\", \n",
    "            \"unique_id_column_name\": \"unique_id\",\n",
    "            \"blocking_rules_to_generate_predictions\": [block_on([\"address_1\", \"zip5\"])],\n",
    "            \"retain_intermediate_calculation_columns\": True,}\n",
    "            \n",
    "linker = SparkLinker(df, settings, spark=spark, set_up_basic_logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7adbb88a-6046-4750-b9c1-92001aa838e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### MODEL TRAINING   \n",
    "🤖`----------------`🤖`--------------------`🤖`--------------------`🤖 </br>\n",
    "\n",
    "Training involves estimating the parameters that comprise Baye's formula to calculate the posterior probability that two records (a,b) are a match.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c652320e-1325-48a5-8f5c-2d791a649982",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾 Parameter Estimation: λ 👾\n",
    "\n",
    "The prior probability that two records drawn at random refer to the same entity `P(Records Match)`. </br> \n",
    "This is our match rate before considering any evidence. It should reflect the overall proportion of true matches in the dataset. </br>\n",
    "\n",
    "**method**: deterministic match rule + recall (% of matches identified by the rule).</br>\n",
    "```\n",
    "💥 Deterministic Rule should be reliable and have a high degree of certainty.\n",
    "💥 rules should not overly restrict the recall, as this could lead to missing true matches and underestimation of λ.\n",
    "💥 Balance the use of rules with the goal of capturing all true matches (high recall). \n",
    "```\n",
    "**interpretation**:</br>\n",
    "* Higher λ: biases the model towards predicting more matches. May be desirable if capturing all true matches (high recall) is more important than minimizing false positives (low precision). May indicate datasets with more duplicates or a higher degree of redundancy\n",
    "* Lower λ: biases the model towards predicting fewer matches. May be preferable if minimizing false positives is a higher priority than capturing all true matches. May suggest datasets with fewer duplicates or more distinct entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a964ef5-e6fc-4088-b245-9d2835c1f30e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimated λ: \n",
    "deterministic_rules = ['l.full_name = r.full_name and l.full_address = r.full_address and l.zip5=r.zip5']\n",
    "linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9c1be9-5df3-48e3-9dd5-82f86565b5c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾 Parameter Estimation: u 👾\n",
    "\n",
    "`P(Observation | Records Do Not Match)`. This is a measure of *coincidence*.\n",
    "\n",
    "**method**: randomly sampling the dataset and observing how often a comparison level agreement occurs. </br> \n",
    "The probability of two random records being a match is very low so we can assume all pairs in the sample are not matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deeb054f-366b-483a-8061-bfd996c52c02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimated u: \n",
    "linker.estimate_u_using_random_sampling(max_pairs=3e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4716f6-dbb3-4b93-817f-2e82ff8ec0db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### 👾 Parameter Estimation: m 👾\n",
    "`P(Observation | Records Match)`. This is the strength of evidence for a match.\n",
    "</br>\n",
    "It is a measure of *data quality/reliability*. How often might a persons information change legitimately (through time) or through data error?\n",
    "\n",
    "\n",
    "**method**:  The Expectation-Maximization algorithm finds the values of \\\\(m\\\\) that best explain the observed evidence in the data by maximizing the likelihood function. \n",
    "1. Initialization: initialize \\\\(m\\\\) with some values\n",
    "2. Expectation (E-step): calculate the probability of a match given the evidence \\\\(γ_i\\\\) and \\\\(m\\\\) .\n",
    "3. Maximization (M-step): update \\\\(m\\\\) by maximizing the likelihood function L = \\\\(∏P(γ_i |m)\\\\) \n",
    "4. Iteration: repeat until \\\\(m\\\\) stabilizes (convergence)\n",
    "\n",
    "The expectation maximisation algorithm is run on a subset of blocked record comparisons. \n",
    "These blocking rules can introduce bias if they disproportionately select certain types of record pairs.\n",
    "```\n",
    "💥 subset the comparison space by selecting pairs of records that are likely to refer to the same entity \n",
    "💥 Try not to bias the estimation by oversampling certain \"types\" of matches or non-matches.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b5d218-b234-4ad5-b09c-d281b560e59a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for name \n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"address_1\", \"zip5\", \"age\"]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd74fde5-e0fe-4ae3-8851-a4cfe351801e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for name \n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"full_address\", \"zip5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cf98a1-e9d6-4d25-87bc-bf37343d2735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for name \n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"full_address\", \"age\", \"zip5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96df34d3-bc3f-43f2-9ec4-f4ff68bad9e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for name \n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"full_address\", \"birth_day\", \"birth_month\", \"zip5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb8fa65-9026-424e-b316-680377d0a87f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for address\n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"first_name\", \"last_name\", \"zip5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c890e14e-38b8-44f4-adeb-bb385a155f75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for age\n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"first_name\", \"last_name\", \"full_address\", \"gender\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb03e99-eba3-4dea-a57f-f91c192cc75b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for gender\n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"first_name\", \"full_address\", \"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a8ee80-5290-40d5-a7e2-b05ca60ffe2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimation of m for gender + age\n",
    "training_session_names =linker.estimate_parameters_using_expectation_maximisation(block_on([\"full_name\", \"full_address\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6e02fa-6d7d-4834-a477-580b4a279879",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### 👾 Model Training Behavior 👾\n",
    "The more stable a model is in the training process, the more reliable the outputs are.\n",
    "\n",
    "`Stability within training session`</br>\n",
    "Speed of convergence is shown in the terminal output during training (above). In general, the fewer iterations required to converge the better.\n",
    "\n",
    "`Stability across training sessions`</br>\n",
    "If the m estimates vary significantly from one run to another, it indicates instability in the estimation process or sensitivity to random variations in the data.\n",
    "```\n",
    "💥 Sensitivity to small changes in the features, blocking rules, or hyperparameters. \n",
    "💥 Data Quality Issues (missing values, errors, inconsistencies)\n",
    "💥 Sample Variability can result in different patterns of agreement and disagreement \n",
    "💥 complex models may exhibit variability due to their flexibility. Look at similarity measures, model assumptions, and optimization algorithms\n",
    "💥 Convergence Issues: failing to converge to a stable solution or getting stuck in local minima\n",
    "💥 Overly complex models may overfit to noise. Overly simplistic models may result in biased or unreliable m estimates.\n",
    "💥 Inconsistent initialization strategies or random initialization conditions \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c463ab7d-59f1-43f3-b3d1-7b7658194213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "linker.parameter_estimate_comparisons_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d119f4-25c5-454a-93c2-9e02926131c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PARAMETER EVALUATION\n",
    "🤖`----------------`🤖`--------------------`🤖`--------------------`🤖 </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91eec13-8437-4b06-b1a8-1321bc81ec2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾 Parameter Interpretation: u 👾\n",
    "\\\\(u\\\\)-values help establish a threshold for determining whether observed agreement between two records is sufficiently indicative of a true match. Records with agreement probabilities above the threshold are classified as potential matches, while those below the threshold are considered non-matches.\n",
    "* Higher \\\\(u\\\\): higher likelihood of observing agreement between non-matching records & suggest that the model should be more conservative in predicting matches to avoid false positives.\n",
    "* Lower \\\\(u\\\\): model can be more confident in predicting matches, as chance agreement between non-matching records is less likely.\n",
    "\n",
    "\n",
    "```\n",
    "💥 Highly driven by cardinality. two different people have the same gender much more often than last name\n",
    "💥 Sparse data or low variability in comparison variables may pose challenges to accurate u estimation, leading to uncertainty in matching decisions.\n",
    "```\n",
    "##### 👾 Parameter Interpretation: m 👾\n",
    "* Higher \\\\(m\\\\)-values: stronger likelihood of agreement between matching records, supporting the decision to classify them as matches. \n",
    "* Lower \\\\(m\\\\)-values: less agreement between matching records, which may require additional evidence or consideration before classifying them as matches.\n",
    "\n",
    "```\n",
    "Reasons for estimated m being too high\n",
    "💥 The comparison variables have high agreement by chance. Can occur if they are not sufficiently discriminative or highly correlated.\n",
    "💥 The comparison variables are sparse or have low variability, the model may overestimate the likelihood of matches\n",
    "💥 If there are errors or inconsistencies in the data, the model may incorrectly assign high probabilities to non-matches\n",
    "💥 If the comparison variables are correlated, the estimated m-probabilities may be biased upwards.\n",
    "\n",
    "Reasons for estimated m-probabilities being too low:\n",
    "💥 If the comparison variables exhibit high variability or noise, the model may underestimate the likelihood of matches\n",
    "💥 comparison variables with limited samples or insufficient information\n",
    "💥 If important variables or features that contribute to identifying matches are missing from the dataset, the model may underestimate the likelihood of matches.\n",
    "💥 If the model is too simplistic and fails to capture the complexities of the data, it may underestimate the likelihood of matches, leading to lower m-probabilities.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123a84be-4a1c-48af-90e3-30f09d94f01b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "linker.m_u_parameters_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8212335b-8333-46c2-8f4b-a535d831fa16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾 Parameter Interpretation: Match Weight 👾\n",
    "The Bayes Factor \\\\(\\frac{m}{u}\\\\) is a ratio of two likelihoods, representing the relative evidence in favor of a match compared to a non match. It provides a quantitative measure of the strength of evidence supporting one hypothesis over another.\n",
    "K acts as a relative multiplier that increases or decreases the overall prediction of whether the records match. </br>\n",
    "K = 5 ⇢`5x more likely to match` We observe this scenario around 5 times more often among matching records than matching records.</br>\n",
    "K = 0.2 ⇢ `5x less likely to match` We observe this scenario around 5 times more often among non-matching records than matching records. </br>\n",
    "\n",
    "strong positive match weights are only possible with low `u`, implying high cardinality. \n",
    "strong negative match weights only possible with low `m`, which in turn implies high data quality.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b548388d-e8b6-4c0e-9f89-ff7780d9bc43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Model Parameters\n",
    "linker.match_weights_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a735298e-6fce-4007-915b-ce5fb952cedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MODEL OUTPUTS\n",
    "🤖`----------------`🤖`--------------------`🤖`--------------------`🤖 </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3159f1d3-b09e-4f6b-9f42-ca5ca6b003ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾 Save Model 👾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2a1b6c-1710-47d1-bb4a-1ebed69b24b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 💾 save model\n",
    "settings = linker.save_model_to_json(f'{MODEL}', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53b4033-f0de-4580-ba9b-d326ed28e5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 👾 Model Predictions 👾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191dd268-d753-47ad-a23a-6767bd6799bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 👾 GENERATE PREDICTIONS 👾\n",
    "predictions = linker.predict(threshold_match_probability=0.2)\n",
    "df_predictions = predictions.as_spark_dataframe()\n",
    "display(predictions.as_pandas_dataframe(limit=10000))\n",
    "# 💾 save predictions\n",
    "df_predictions.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{PREDICTION_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9743ad0-6480-46dc-aab8-d87785e250c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "records_to_view  = predictions.as_record_dict(limit=50)\n",
    "linker.waterfall_chart(records_to_view, filter_nulls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd075e03-2379-42a5-9514-92439e7161f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'splink.spark'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2143697984260545>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 👾 IMPORTS 👾\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msplink\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjar_location\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m similarity_jar_location\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, types\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'splink.spark'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 👾 GENERATE CLUSTERS 👾\n",
    "clusters = linker.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.2)\n",
    "clusters_df = clusters.as_spark_dataframe()\n",
    "# 💾 save clusters\n",
    "clusters_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CLUSTER_TABLE}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2143697984260546,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "✨ Entity Resolution Model ✨",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
